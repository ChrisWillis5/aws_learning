1. Create a python function (Script) to generate data : Sample data and store in a CSV file in you laptop. [50000 records]
The file name: order_data_20230201.csv


orderid,brand_name,product_name,sales_ammount,sales_date
orderid: Unique id with 7 digit number in sequence[Pandas function random number]
brand_name:['Apple','Samsung','Nokia']
product_name is Key value pair
product_name:{'Apple':['iphon11','iphone12','iphone13','iphoneSE','IpadMax','IpadMini','laptop256','Macbook512'],
'Samsung':['galaxy10','galaxy11','galaxy12','galaxy13','watch320','watch340'],
'Nokia':['Nk320',''Nk400',''Nk500']
sales_ammount : random ammout with (5,2) decimal.
sales_datetime : 2023-02-01 to 2023-02-20

Finally data should be like this
1002346,Apple,iphon11,450,2023-02-01 10:30
1002246,Apple,iphon11,460,2023-02-01 11:20
1002546,Apple,iphoneSE,350,2023-02-01 12:20

Generate 20 files. Copy them into s3 into one bucket. [[ lets say bucket1/folder1 ]]
1. Create a dag to have 4 task


Task 1.
use date parameter from dag.
date=20230201
1. Check file exists in s3 bucket1/folder1/order_data_{date}.csv

Task 2:
2. Copy the file to another folder with
2023-02-01/order_data_20230201.csv
2023-02-02/order_data_20230202.csv

Task3:
3. Run the crawler to add the partition as dataset_date

Task4 .
Run a glue job
get the total ammount by brand_name,product_name,dataset_date and load into file. Makse sure you have dynamic partition by brand_name
